import pandas as pd
import logging
from typing import Dict, Any, Optional
import base64
from pandasai import SmartDataframe
from pandasai.llm.base import LLM
from pandasai.core.prompts.base import BasePrompt
from ..config import settings
from ..services.ollama_service import ollama_service

logger = logging.getLogger(__name__)

class PandasAIOllamaLLM(LLM):
    """
    Custom PandasAI LLM implementation using our existing OllamaService.
    Adapts PandasAI's LLM interface to our Ollama infrastructure.
    """
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.model = settings.OLLAMA_MODEL
        
    def type(self) -> str:
        return "ollama"

    def call(self, instruction: BasePrompt, context: Any = None) -> str:
        """
        Execute the LLM with given prompt.
        PandasAI passes a BasePrompt object which has to_string() method.
        """
        prompt_text = instruction.to_string()
        
        # Log the prompt for debugging
        # logger.info(f"PandasAI Prompt: {prompt_text[:200]}...")
        
        try:
            # Bypass async wrapper and call Ollama directly synchronously
            # This avoids nested loop/thread issues since PandasAI is sync
            import ollama
            
            # Get prompt configuration
            system_prompt = """You are a data analysis assistant. 
            RULES:
            1. Output ONLY python code.
            2. The dataframe is already loaded as `dfs[0]`. Use `dfs[0]` directly.
            3. DO NOT use `execute_sql_query` or any SQL. DO NOT use `duckdb`.
            4. If asked to visualize/plot, you MUST use matplotlib/seaborn and save the plot to 'exports/charts/temp_chart_<uuid>.png'.
            5. Do not wrap code in markdown blocks.
            6. The LAST line of code must be the path to the saved image file as a string. Example: 'exports/charts/plot.png'"""
            full_prompt = f"System: {system_prompt}\n\nUser: {prompt_text}"
            
            logger.info(f"Sending prompt to Ollama (Sync): {full_prompt[:100]}...")
            
            client = ollama.Client(host=settings.OLLAMA_HOST)
            
            options = {
                "temperature": 0.1,
                "num_predict": 2048,
                # "stop": ["```"] # Don't stop early for PandasAI as it might explain
            }
            
            response_obj = client.generate(
                model=settings.OLLAMA_MODEL,
                prompt=full_prompt,
                options=options
            )
            
            response = response_obj['response']
            logger.info(f"PandasAI LLM Response:\n{response}")
            return response
        except Exception as e:
            logger.error(f"PandasAI LLM Call Error: {e}")
            raise e

class PandasAIService:
    """
    Service wrapper for PandasAI library.
    Provides natural language analysis and visualization capabilities "exactly like pandas-ai".
    """
    
    def __init__(self):
        self.llm = self._configure_llm()
    
    def _configure_llm(self):
        """Configure the LLM for PandasAI"""
        # Always use our custom Ollama wrapper for consistency
        return PandasAIOllamaLLM()

    def analyze(self, df: pd.DataFrame, query: str) -> Dict[str, Any]:
        """
        Analyze dataframe using PandasAI.
        
        Args:
            df: Pandas DataFrame
            query: User's natural language query
            
        Returns:
            Dict containing:
            - answer: Text response
            - plot_image: Base64 encoded image (if generated)
            - code: The code generated by PandasAI
        """
        try:
            # Configure SmartDataframe
            sdf = SmartDataframe(
                df, 
                config={
                    "llm": self.llm,
                    "save_charts": True, # Ensure charts are saved to disk temporarily
                    "save_charts_path": settings.UPLOAD_DIR, # Use upload dir for temp charts
                    "custom_whitelisted_dependencies": ["matplotlib", "seaborn", "plotly"],
                    "verbose": True
                }
            )
            
            # Execute chat
            response = sdf.chat(query)
            logger.info(f"PandasAI Execution Result Type: {type(response)}")
            logger.info(f"PandasAI Execution Result: {response}")
            
            plot_image = None
            answer = ""
            
            # Clean response if it's a string
            if isinstance(response, str):
                response = response.strip()
                # Check if it looks like a dict string
                if response.startswith("{") and response.endswith("}"):
                    try:
                        import ast
                        response_dict = ast.literal_eval(response)
                        if isinstance(response_dict, dict) and 'value' in response_dict:
                            response = response_dict['value']
                    except Exception:
                        pass

            # Handle response specifically looking for image paths
            is_image_path = False
            image_path = ""
            
            if isinstance(response, str):
                if response.lower().endswith('.png') or response.lower().endswith('.jpg') or response.lower().endswith('.jpeg'):
                    is_image_path = True
                    image_path = response
                elif isinstance(response, dict) and response.get('type') == 'plot':
                    is_image_path = True
                    image_path = response.get('value')

            # Check for plot file
            if is_image_path:
                 try:
                    import os
                    # Clean path
                    if image_path.startswith("'") or image_path.startswith('"'):
                        image_path = image_path.strip("'\"")
                        
                    logger.info(f"PandasAI returned path: {image_path}")
                    logger.info(f"Current CWD: {os.getcwd()}")
                    
                    file_path = image_path
                    if not os.path.exists(file_path):
                        # Try relative to CWD
                        file_path = os.path.join(os.getcwd(), image_path)
                        logger.info(f"Tried absolute path: {file_path}")
                    
                    if not os.path.exists(file_path):
                        # Fallback: check exports/charts explicitly if response was just filename
                        if "exports/charts" not in image_path:
                             fallback = os.path.join(os.getcwd(), "exports/charts", os.path.basename(image_path))
                             if os.path.exists(fallback):
                                 file_path = fallback
                                 logger.info(f"Found at fallback: {file_path}")
                    
                    if not os.path.exists(file_path):
                        logger.error(f"Chart file REALLY not found at {image_path} or {file_path}")
                        # Check recursively in current dir for any png with that name
                        filename = os.path.basename(image_path)
                        for root, dirs, files in os.walk(os.getcwd()):
                            if filename in files:
                                file_path = os.path.join(root, filename)
                                logger.info(f"Found via walk: {file_path}")
                                break
                    
                    if not os.path.exists(file_path):
                         answer = f"Chart generated at {image_path} but could not be loaded."
                    else:
                        with open(file_path, "rb") as image_file:
                            plot_image = base64.b64encode(image_file.read()).decode('utf-8')
                        answer = "I have generated the chart for you."
                 except Exception as e:
                    logger.error(f"Failed to read chart file: {e}")
                    answer = "Generated chart but failed to load it."
            
            elif isinstance(response, pd.DataFrame):
                answer = response.to_markdown()
                
            elif isinstance(response, SmartDataframe):
                 # Sometimes returns itself?
                 answer = "Operation completed."
            
            else:
                answer = str(response)

            return {
                "success": True,
                "answer": answer,
                "plot_image": plot_image,
                "code": sdf.last_code_executed, # Verify if this property exists in v3
                "raw_response": str(response)
            }
            
        except Exception as e:
            logger.error(f"PandasAI Analysis Error: {e}", exc_info=True)
            try:
                 logger.error(f"Last Code Executed:\n{sdf.last_code_executed}")
            except:
                 pass
            return {
                "success": False,
                "error": str(e)
            }

pandas_ai_service = PandasAIService()
