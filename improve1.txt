PROJECT CONTEXT
I have an AI-powered business analytics platform (similar to CamelAI) that converts natural language questions into SQL queries and visualizations. Currently, the results are too basic. I need to transform it into an intelligent data analyst that provides comprehensive, iterative analysis with detailed insights.

CORE PROBLEMS TO SOLVE
Problem 1: Single-Shot Query Execution

Current: User asks question → Generate 1 SQL query → Show results → Done
Needed: User asks question → Plan analysis → Execute 3-5 related queries → Synthesize insights → Present comprehensive answer

Problem 2: No Narrative Insights

Current: Just showing SQL and raw data tables
Needed: Generate human-readable analysis explaining what the data means, trends, patterns, and actionable recommendations

Problem 3: Basic Prompt Engineering

Current: Simple schema + question prompts
Needed: Rich context including sample data, statistics, business logic, and multi-stage reasoning


IMPLEMENTATION TASKS
TASK 1: Create Agentic Query Pipeline
File to Create/Modify: backend/app/services/query_agent.py
Requirements:
python# Implement a multi-step analysis workflow:

class QueryAgent:
    async def analyze_question(self, question, dataset):
        # Step 1: Planning Phase
        analysis_plan = await self.create_analysis_plan(question, dataset)
        # Returns: List of 3-5 sub-questions to explore
        
        # Step 2: Exploratory Queries
        exploratory_results = []
        for sub_question in analysis_plan:
            sql = await self.generate_sql(sub_question, dataset)
            results = await self.execute_query(sql)
            exploratory_results.append(results)
        
        # Step 3: Main Analysis Query
        context = self.build_context(exploratory_results)
        main_sql = await self.generate_main_query(question, dataset, context)
        main_results = await self.execute_query(main_sql)
        
        # Step 4: Insight Generation
        insights = await self.generate_insights(
            question, 
            exploratory_results, 
            main_results
        )
        
        # Step 5: Visualization Selection
        charts = await self.select_visualizations(main_results)
        
        return {
            'sql': main_sql,
            'results': main_results,
            'insights': insights,  # Narrative text
            'charts': charts,
            'exploratory_queries': exploratory_results
        }
Key Features:

Chain multiple LLM calls (5-7 calls per user question)
Each step feeds context into the next
Store intermediate results for synthesis


TASK 2: Enhanced Prompt Templates
File to Create: backend/app/prompts/analysis_prompts.py
Requirements:
Create separate prompt templates for each stage:
Template 1: Planning Prompt
pythonPLANNING_PROMPT = """
You are a senior data analyst. A user asked: "{question}"

Database Schema:
{schema_with_sample_data}

Statistics:
{column_statistics}

Your task: Create a step-by-step analysis plan.
Break down this question into 3-5 sub-questions that need to be explored.

Output format (JSON):
{
  "understanding": "What the user is asking for",
  "sub_questions": [
    "What is the overall data volume?",
    "What are the key categories?",
    "What are the trends over time?",
    ...
  ],
  "required_metrics": ["metric1", "metric2"],
  "suggested_visualizations": ["line_chart", "bar_chart"]
}
"""
Template 2: SQL Generation Prompt
pythonSQL_GENERATION_PROMPT = """
Generate SQL to answer: "{sub_question}"

Context from previous queries:
{previous_results_summary}

Database Schema:
{detailed_schema}

Requirements:
- Use CTEs for complex logic
- Add SQL comments explaining each section
- Handle NULL values
- Optimize for performance
- Limit results to 1000 rows

Output format (JSON):
{
  "sql": "SELECT ...",
  "explanation": "This query does X by...",
  "complexity": "simple|medium|complex"
}
"""
Template 3: Insight Generation Prompt
pythonINSIGHT_PROMPT = """
You are explaining data analysis results to a business user.

User's Question: "{question}"

Query Results:
{formatted_results}

Additional Context:
{exploratory_findings}

Write a comprehensive analysis following this structure:

1. **Summary** (2-3 sentences): High-level answer to the question

2. **Key Findings** (bullet points):
   - Most important insights
   - Notable trends or patterns
   - Unexpected discoveries

3. **Detailed Analysis** (2-3 paragraphs):
   - Explain what the data shows
   - Provide context and comparisons
   - Highlight relationships between variables

4. **Recommendations** (if applicable):
   - Actionable next steps
   - Areas needing attention
   - Opportunities identified

Write in clear, business-friendly language. Avoid technical jargon.
"""

TASK 3: Context Enrichment System
File to Modify: backend/app/services/schema_service.py
Add These Functions:
pythonasync def enrich_schema_context(dataset_id):
    """
    Don't just send table schema. Enrich with:
    - Sample data (first 5 rows)
    - Column statistics (min, max, avg, distinct count, null %)
    - Data types with examples
    - Relationships between columns
    - Common patterns in the data
    """
    
    schema = get_basic_schema(dataset_id)
    
    # Add sample data
    schema['samples'] = get_sample_rows(dataset_id, limit=5)
    
    # Add statistics for each column
    for column in schema['columns']:
        schema['statistics'][column] = {
            'distinct_count': calculate_distinct(column),
            'null_percentage': calculate_nulls(column),
            'min_value': get_min(column),
            'max_value': get_max(column),
            'data_type': infer_type(column),
            'sample_values': get_unique_samples(column, limit=10)
        }
    
    return schema

TASK 4: Smart Visualization Engine
File to Create: backend/app/services/visualization_service.py
Requirements:
pythonclass VisualizationSelector:
    def select_chart_types(self, query_results, question):
        """
        Automatically determine best chart types based on:
        - Number of columns in results
        - Data types (numeric, categorical, temporal)
        - Question intent (trend, comparison, distribution)
        - Result size
        
        Return 2-3 complementary visualizations
        """
        
        charts = []
        
        # Analyze result structure
        has_time_column = self.detect_temporal_column(query_results)
        has_categories = self.detect_categorical_columns(query_results)
        numeric_columns = self.get_numeric_columns(query_results)
        
        # Selection logic
        if has_time_column and len(numeric_columns) > 0:
            charts.append({
                'type': 'line',
                'x': time_column,
                'y': numeric_columns[0],
                'title': 'Trend Over Time'
            })
        
        if has_categories and len(numeric_columns) > 0:
            charts.append({
                'type': 'bar',
                'x': category_column,
                'y': numeric_columns[0],
                'title': 'Comparison by Category',
                'sort': 'descending',
                'limit': 10  # Top 10 only
            })
        
        # Always include summary statistics
        charts.append({
            'type': 'metric_cards',
            'metrics': self.calculate_key_metrics(query_results)
        })
        
        return charts

TASK 5: Conversation Memory
File to Modify: backend/app/routes/analytics.py
Requirements:
python# Store conversation history in database or session
# Include in every prompt:

async def get_conversation_context(session_id):
    """
    Retrieve last 5 Q&A pairs from this session
    Format for LLM context
    """
    
    history = db.query(QueryHistory)\
        .filter(QueryHistory.session_id == session_id)\
        .order_by(QueryHistory.created_at.desc())\
        .limit(5)\
        .all()
    
    context = []
    for entry in history:
        context.append({
            'question': entry.question,
            'sql': entry.sql_generated,
            'key_findings': entry.insights_summary
        })
    
    return context

# Include this in every prompt:
# "Previous conversation context: {context}"

TASK 6: Response Formatting
File to Modify: backend/app/services/response_formatter.py
Requirements:
pythondef format_final_response(analysis_result):
    """
    Structure the response to match CamelAI format
    """
    return {
        'understanding': analysis_result['understanding'],
        'approach': analysis_result['analysis_plan'],
        'exploratory_steps': [
            {
                'question': step['question'],
                'finding': step['summary']
            }
            for step in analysis_result['exploratory_results']
        ],
        'sql_query': {
            'query': analysis_result['main_sql'],
            'explanation': analysis_result['sql_explanation']
        },
        'results': {
            'table': analysis_result['data'],
            'row_count': len(analysis_result['data'])
        },
        'visualizations': analysis_result['charts'],
        'analysis': {
            'summary': analysis_result['insights']['summary'],
            'key_findings': analysis_result['insights']['findings'],
            'detailed_analysis': analysis_result['insights']['analysis'],
            'recommendations': analysis_result['insights']['recommendations']
        }
    }

TASK 7: Frontend Display Updates
File to Modify: frontend/src/pages/Analytics.tsx
Requirements:
Update the UI to display all sections:

Show "Analysis in Progress" loader with steps
Display exploratory queries in expandable sections
Show main SQL with syntax highlighting
Render multiple charts side-by-side
Display insights in formatted text blocks
Add copy buttons for SQL
Show reasoning process transparently


TASK 8: Error Handling & Retry Logic
File to Create: backend/app/services/query_validator.py
Requirements:
pythonasync def execute_with_retry(sql, max_attempts=3):
    """
    If SQL fails:
    1. Parse error message
    2. Send error back to LLM
    3. Ask LLM to fix the SQL
    4. Retry execution
    """
    
    for attempt in range(max_attempts):
        try:
            results = execute_sql(sql)
            
            # Validate results make sense
            if len(results) == 0:
                suggestion = await llm_suggest_alternative(sql)
                return {'warning': 'No results', 'suggestion': suggestion}
            
            return results
            
        except SQLError as e:
            if attempt < max_attempts - 1:
                sql = await llm_fix_sql(sql, str(e))
            else:
                return {'error': str(e), 'failed_sql': sql}

PRIORITY ORDER FOR IMPLEMENTATION
Phase 1 (Week 1-2):

Task 1: Agentic Query Pipeline ⭐⭐⭐
Task 2: Enhanced Prompts ⭐⭐⭐
Task 6: Response Formatting ⭐⭐

Phase 2 (Week 3-4):
4. Task 3: Context Enrichment ⭐⭐
5. Task 4: Smart Visualizations ⭐⭐
6. Task 7: Frontend Updates ⭐⭐
Phase 3 (Week 5+):
7. Task 5: Conversation Memory ⭐
8. Task 8: Error Handling ⭐

CRITICAL CONFIGURATION CHANGES
Ollama Model Settings:
python# In backend/app/config.py
OLLAMA_CONFIG = {
    'model': 'llama3.1:70b',  # Use larger model if possible
    'temperature': 0.3,  # Lower for SQL, higher for insights
    'num_ctx': 8192,  # Larger context window
    'num_predict': 2048  # Allow longer responses
}

# Use different temperatures for different tasks:
# - SQL Generation: 0.1 (precise)
# - Planning: 0.5 (creative)
# - Insights: 0.7 (more creative)

TESTING CHECKLIST
After implementation, test with these questions:

"What are the top 10 products by revenue?"
"Show me sales trends over the last 6 months"
"Which customers are at risk of churning?"
"Compare Q1 vs Q2 performance"
"What's the average order value by region?"

Expected output for each should include:

Multiple exploratory queries shown
Detailed narrative explanation
2-3 different visualizations
Key insights bullet points
Actionable recommendations