COMPLETE BACKEND ARCHITECTURE GUIDE
1. CORE ARCHITECTURE COMPONENTS
A. Multi-Agent Agentic System (Not Single Prompt)
Don't use simple one-shot prompting. You need an iterative agent loop with multiple reasoning steps:

Query Understanding Agent: Determines user intent and validates against dataset
Schema Mapping Agent: Builds semantic understanding of your data
SQL Generation Agent: Creates queries with multi-step reasoning
Validation Agent: Self-corrects SQL errors
Visualization Agent: Generates appropriate chart types
Explanation Agent: Provides business insights

B. Database Mapping System (Critical for 80% Accuracy)
CamelAI increased query accuracy from 30% to 80% through proper database mapping Camelai. You need:

Automatic schema extraction: Column names, data types, relationships, sample values
Semantic layer: Business definitions (what is "active user"? "revenue"?)
Example query library: Store successful queries as reference patterns
Column relationship discovery: Foreign keys, join patterns
Data profiling: Min/max values, distributions, null percentages
Business glossary: Map technical columns to business terms

2. TECHNOLOGY STACK
Backend Core:

Python 3.11+
FastAPI (current framework, keep it)
PostgreSQL for metadata storage
DuckDB (CRITICAL - for in-memory SQL execution on uploaded files)

LLM Provider & Models:
For OpenAI API:

Use GPT-4o (gpt-4o-2024-08-06) - NOT GPT-3.5
GPT-4o is required for:

Complex SQL generation with CTEs and window functions
Multi-step reasoning
Function calling for tool use
Better schema understanding



Better Alternative: Claude API (Anthropic)

Claude Sonnet 4 (claude-sonnet-4-20250514) - BEST for SQL generation
Superior reasoning for complex analytics
CamelAI uses Claude Sonnet for text-to-SQL conversion Camelai
Better at self-correction
Costs less than GPT-4

Data Processing:

Pandas for data manipulation
DuckDB for SQL execution on uploaded files (faster than SQLite)
SQLAlchemy for database operations
Polars (optional) for large datasets

Visualization:

Plotly (generates Python code server-side, sends JSON to frontend)
Support: Line, Bar, Pie, Scatter, Heatmap, Box plots

3. TECHNICAL IMPLEMENTATION STRATEGY
A. Semantic Layer & RAG System
Build a knowledge base for each uploaded dataset:
1. On file upload:
   - Extract schema (columns, types)
   - Generate statistics (value ranges, unique counts, null percentages)
   - Sample 100-1000 rows
   - Store embeddings of column names + descriptions
   
2. Create semantic index:
   - Use text-embedding-3-small (OpenAI) or Voyage AI
   - Store in vector DB (Qdrant, Chroma, or PostgreSQL pgvector)
   - Index: column names, sample values, business definitions
   
3. Build query template library:
   - Store successful SQL queries with natural language descriptions
   - Use as few-shot examples
B. Query Generation Pipeline
The process uses an iterative agent loop with multi-step reasoning Camelai. Implement this flow:
1. Intent Classification
   - Is question answerable with this dataset?
   - Question type: aggregation, filtering, trend, comparison, etc.
   
2. Schema Retrieval (RAG)
   - Retrieve relevant tables/columns using embeddings
   - Get example queries similar to current question
   - Pull business definitions
   
3. SQL Generation (with Chain-of-Thought)
   Prompt structure:
   - System: "You are a data analyst expert..."
   - Context: Schema + samples + definitions + similar queries
   - Task: "Generate SQL for: {user_question}"
   - Format: "Think step-by-step, then write SQL"
   
4. SQL Validation & Self-Correction
   - Execute SQL in sandbox (DuckDB)
   - If error: Parse error message
   - Feed back to LLM: "This query failed with error X. Fix it."
   - Max 3 correction attempts
   
5. Result Processing
   - Check if results are empty
   - If empty: Ask LLM to refine query or ask clarifying question
   - If results exist: Proceed to visualization
   
6. Visualization Generation
   - Analyze result structure (time series? categories? distributions?)
   - Generate Plotly code
   - Execute Python code in isolated environment
   - Return Plotly JSON
   
7. Insight Generation
   - Send results + query to LLM
   - Prompt: "Analyze these results and provide 3-5 business insights"
   - Return explanation in natural language
C. Prompt Engineering (DON'T Fine-tune)
DO NOT fine-tune. Use advanced prompting:
1. System Prompt Template:
You are an expert SQL analyst and data scientist. Your job is to:
1. Understand user questions about data
2. Generate accurate SQL queries
3. Provide clear explanations
4. Suggest visualizations

When generating SQL:
- Use standard SQL syntax compatible with DuckDB
- Always use table aliases
- Include comments explaining complex logic
- Validate column names against schema
- Use appropriate aggregations and filters

If a question cannot be answered with available data, clearly state why.
2. Few-Shot Examples:
Include 3-5 high-quality examples:
Example 1:
Question: "What are the top 5 products by revenue?"
SQL: SELECT product_name, SUM(price * quantity) as revenue 
     FROM sales 
     GROUP BY product_name 
     ORDER BY revenue DESC 
     LIMIT 5
3. Schema Context (Critical):
Available tables and columns:
- sales: [id, product_id, customer_id, quantity, price, date]
  - date range: 2023-01-01 to 2024-12-31
  - 50,000 total rows
  - product_id links to products.id
  
- products: [id, name, category, cost]
  - 500 unique products
  
- customers: [id, name, email, signup_date]
4. Chain-of-Thought Prompting:
Before writing SQL, think through:
1. What is the user asking for?
2. Which tables/columns are needed?
3. What aggregations or filters are required?
4. What is the expected output format?

Then write the SQL query.
4. QUESTION VALIDATION SYSTEM
Build an intent validator to reject irrelevant questions:
python# Step 1: Check if question is data-related
def is_question_relevant(question, dataset_schema):
    prompt = f"""
    Dataset contains columns: {dataset_schema}
    
    User question: "{question}"
    
    Can this question be answered using this dataset?
    Reply with JSON: {{"answerable": true/false, "reason": "explanation"}}
    """
    
    response = llm.generate(prompt)
    return parse_json(response)

# Step 2: If not answerable, return clear message
if not is_relevant:
    return {
        "error": "This question cannot be answered with the current dataset.",
        "reason": "Your dataset contains sales data, but you're asking about weather patterns.",
        "suggestion": "Try asking about sales trends, customer behavior, or product performance."
    }
5. ERROR HANDLING & SELF-CORRECTION
Implement robust error recovery:
pythondef execute_query_with_retry(sql, max_attempts=3):
    for attempt in range(max_attempts):
        try:
            result = duckdb.execute(sql).fetchall()
            return result
        except Exception as e:
            if attempt == max_attempts - 1:
                raise
            
            # Ask LLM to fix
            correction_prompt = f"""
            This SQL query failed:
```sql
            {sql}
```
            
            Error: {str(e)}
            
            Fix the query and return corrected SQL only.
            """
            
            sql = llm.generate(correction_prompt)
6. VISUALIZATION GENERATION
CamelAI runs Python code execution in a secure environment for visualizations Camelai. Implement:
pythondef generate_visualization(query_results, question):
    # Analyze result structure
    prompt = f"""
    Query results: {query_results[:5]} (showing first 5 rows)
    Original question: "{question}"
    
    Generate Python code using Plotly to visualize this data.
    Choose appropriate chart type (line, bar, pie, scatter).
    Return only executable Python code.
    
    Available libraries: plotly, pandas
    Result is already in 'df' DataFrame.
    """
    
    python_code = llm.generate(prompt)
    
    # Execute in sandbox
    plotly_json = execute_python_safely(python_code, df)
    
    return plotly_json
7. RECOMMENDED TECH STACK SUMMARY
yamlCore Backend:
  - FastAPI (keep current)
  - Python 3.11+
  - Uvicorn/Gunicorn
  
LLM:
  - Primary: Claude Sonnet 4 (Anthropic API)
  - Fallback: GPT-4o (OpenAI API)
  - Embeddings: text-embedding-3-small (OpenAI)
  
Data Processing:
  - DuckDB (in-memory SQL, REQUIRED)
  - Pandas
  - Polars (for large files)
  
Vector Search (for RAG):
  - Qdrant (simple, fast) OR
  - PostgreSQL with pgvector OR
  - Chroma (embedded)
  
Visualization:
  - Plotly (server-side Python â†’ JSON)
  
Task Queue:
  - Celery + Redis (for long-running queries)
  
Caching:
  - Redis (cache query results)
  
Monitoring:
  - LangSmith or LangFuse (LLM tracing)
  - Sentry (error tracking)
```

### **8. KEY IMPLEMENTATION STEPS**

**Phase 1: Foundation (Week 1)**
1. Switch from Ollama to Claude Sonnet 4 API
2. Implement DuckDB for file-based SQL execution
3. Build schema extraction pipeline

**Phase 2: Core Intelligence (Week 2)**
4. Implement semantic layer with embeddings
5. Build RAG system for schema retrieval
6. Create query template library

**Phase 3: Query Generation (Week 3)**
7. Build multi-step agent with self-correction
8. Implement question validation
9. Add error handling and retry logic

**Phase 4: Visualization (Week 4)**
10. Build safe Python execution sandbox
11. Implement Plotly code generation
12. Add insight generation

**Phase 5: Polish (Week 5)**
13. Add caching layer
14. Implement rate limiting
15. Build monitoring and logging

### **9. CRITICAL SUCCESS FACTORS**

1. **Don't fine-tune** - Advanced prompting + RAG is sufficient and more flexible
2. **Use Claude Sonnet 4** - Better than GPT-4 for SQL, cheaper
3. **Build semantic layer** - This is what gets you from 30% to 80% accuracy
4. **Implement self-correction** - SQL will fail; build retry logic
5. **Use DuckDB** - Essential for fast in-memory SQL on uploaded files
6. **Store successful queries** - Build a library of working examples
7. **Validate questions early** - Reject irrelevant questions immediately
8. **Multi-step reasoning** - Don't try to do everything in one prompt

### **10. COST OPTIMIZATION**
```
- Cache query results (Redis): 70% cost reduction
- Use Claude instead of GPT-4: 80% cheaper for same quality
- Batch embed requests: text-embedding-3-small is cheap
- Limit token usage with smart context selection (RAG)
- Stream responses for better UX (doesn't save cost but feels faster)

Expected costs (1000 queries/day):
- Claude Sonnet 4: ~$30-50/day
- Embeddings: ~$2/day
- Total: ~$1,000-1,500/month at scale